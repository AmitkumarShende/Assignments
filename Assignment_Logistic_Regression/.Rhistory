trend+I(trend^2)+I(trend^3),data=Computer_data1[-c(1441,1701),])
summary(FinalModel) #Adjusted R2 Value = 0.813
Profit_Predict <- predict(FinalModel)
View(Profit_Predict)
finplot <- Computer_data1[-c(1441,1701),]
View(finplot)
plot1 <- cbind(finplot$price, Profit_Predict)
pairs(plot1)
pairs(plot1)
Final <- cbind(speed,hd,ram,screen,cd,multi,premium,ads,trend,price,Profit_Predict)
pairs(Final)
library("MASS")
stepAIC(Model.Computer_dataLog) # backward
library("MASS")
stepAIC(Model.Computer_dataLog) # backward
stepAIC(FinalModel) # backward
Startups <- read.csv(file.choose())
Startups <- read.csv(file.choose())
View(Startups)
class(Startups)
# To Transform the data from Character to Numeric
library(plyr)
Startups$State <- revalue(Startups$State,
c("New York"="0", "California"="1", "Florida"="2"))
attach(Startups)
Startups <- cbind(RD_Spend=R.D.Spend,Administration,Marketing_Spend=Marketing.Spend,State,Profit)
Startups <- as.data.frame(Startups)
attach(Startups) # Basically to avoid reference of Data Set name(Startups) in this report.
summary(Startups)
plot(R.D.Spend, Profit)
plot(Administration, Profit)
plot(Marketing.Spend, Profit)
plot(State, Profit)
windows()
# 7. Find the correlation between Output (Profit) & inputs (R.D Spend, Administration, Marketing, State) - SCATTER DIAGRAM
pairs(Startups)
# 8. Correlation coefficient - Strength & Direction of correlation
cor(Startups)
# The Linear Model of interest
Model.Startups <- lm(Profit~RD_Spend+Administration+Marketing_Spend+State)
summary(Model.Startups)
Model.Startups1 <- lm(Profit~RD_Spend+log(Administration))
summary(Model.Startups1)
### Scatter plot matrix with Correlations inserted in graph
panel.cor <- function(x, y, digits=2, prefix="", cex.cor)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r = (cor(x, y))
txt <- format(c(r, 0.123456789), digits=digits)[1]
txt <- paste(prefix, txt, sep="")
if(missing(cex.cor)) cex <- 0.4/strwidth(txt)
text(0.5, 0.5, txt, cex = cex)
}
pairs(Startups, upper.panel=panel.cor,main="Scatter Plot Matrix with Correlation Coefficients")
### Partial Correlation matrix - Pure correlation between the variables
# install.packages("corpcor")
library(corpcor)
cor2pcor(cor(Startups))
# install.packages("mvinfluence")
library(mvinfluence)
library(car)
# It is better to delete a single observation rather than entire variable to get rid of collinearity problem
# Deletion Diagnostics for identifying influential variable
influence.measures(Model.Startups)
influenceIndexPlot(Model.Startups, id.n=3) # Index Plots of the influence measures
influencePlot(Model.Startups, id.n=3) # A user friendly representation of the above
# Logarthimic Transformation
Model.Startups_Log<-lm(Profit~RD_Spend+log(Administration)+Marketing_Spend+log(State),data=Startups[-c(49,50),])
summary(Model.Startups_Log) #Adjusted R2 Value = 0.9591
confint(Model.Startups_Log,level=0.95)
predict(Model.Startups_Log,interval="predict")
Model.Startups_Fin1<-lm(Profit~RD_Spend+Administration+Marketing_Spend+State,data=Startups[-c(49,50),])
summary(Model.Startups_Fin1) # Adjusted R2 Value is 0.9567
# Exponential Transformation :
Model.Startups_exp<-lm(log(Profit)~RD_Spend+Administration+Marketing_Spend+State,data=Startups[-c(49,50),])
summary(Model.Startups_exp)  #Adjusted R2 Value is 0.9182
Model.Startups_exp1<-lm(log(Profit)~RD_Spend+Marketing_Spend,data=Startups[-c(49,50),])
summary(Model.Startups_exp1)
# Quad Model
Model.Startups_Quad <- lm(Profit~RD_Spend+I(RD_Spend^2)+Administration+I(Administration^2)
+Marketing_Spend+I(Marketing_Spend^2)+State+I(State^2),data=Startups[-c(49,50),])
summary(Model.Startups_Quad)  #Adjusted R2 value is 0.9567
confint(Model.Startups_Quad,level=0.95)
predict(Model.Startups_Quad,interval="predict")
Model.Startups_Quad1 <- lm(Profit~RD_Spend+I(RD_Spend^2)+Marketing_Spend+I(Marketing_Spend^2)
,data=Startups[-c(49,50),])
summary(Model.Startups_Quad1)  #Adjusted R2 value is 0.9567
summary(Model.Startups_Quad1)  #Adjusted R2 value is 0.9585
# Poly Modal
Model.Startups_Poly <- lm(Profit~RD_Spend+I(RD_Spend^2)+I(RD_Spend^3)+
Administration+I(Administration^2)+I(Administration^3)+
Marketing_Spend+I(Marketing_Spend^2)+I(Marketing_Spend^3)+
State+I(State^2)+I(State^3),data=Startups[-c(49,50),])
summary(Model.Startups_Poly) #Adjusted R Square Value is 0.9569
Model.Startups_Poly1 <- lm(Profit~RD_Spend+I(RD_Spend^2)+I(RD_Spend^3)+
Marketing_Spend+I(Marketing_Spend^2)+I(Marketing_Spend^3)
,data=Startups[-c(49,50),])
summary(Model.Startups_Poly1) #Adjusted R Square Value is 0.9601
### Variance Inflation Factors is a formal way to check for collinearity
vif(Model.Startups_Log)  # VIF is > 10 => collinearity
avPlots(Model.Startups_Log, id.n=2, id.cex=0.7) # Added Variable Plots
# Final Model
FinalModel<-lm(Profit~RD_Spend+log(Administration)+Marketing_Spend+
log(State),data=Startups[-c(49,50),])
summary(FinalModel) #Adjusted R2 Value = 0.9591
Profit_Predict <- predict(FinalModel,interval="predict")
Final <- cbind(Startups$RD_Spend,Startups$Administration,Startups$Marketing_Spend,
Startups$State,Startups$Profit,Profit_Predict)
View(Final)
# Evaluate model LINE assumptions
plot(FinalModel)# Residual Plots, QQ-Plos, Std. Residuals vs Fitted, Cook's distance
qqPlot(FinalModel, id.n=5) # QQ plots of studentized residuals, helps identify outliers
library("MASS")
qqPlot(FinalModel, id.n=5) # QQ plots of studentized residuals, helps identify outliers
qqPlot(FinalModel, id.n=5) # QQ plots of studentized residuals, helps identify outliers
library("MASS")
stepAIC(FinalModel) # backward
############2 sample T Test ##################
cutlet<-read_excel(file.choose())    # cutlet.xlsx
############2 sample T Test ##################
cutlet<-read.csv(file.choose())    # cutlet.xlsx
View(cutlet)
View(cutlet)
colnames(cutlet)
shapiro.test(Unit.A)
library(nortest)
install.packages("nortest")
library(nortest)
shapiro.test(Unit.A)
shapiro.test(cutlet$Unit.A)
shapiro.test(cutlet$unit.B)
shapiro.test(cutlet$Unit.B)
var.test(cutlet$Unit.A,cutlet$Unit.B)#variance test
library(readxl)
t.test(y1,y2,alternative = "two.sided",conf.level = 0.95,paired = TRUE)
y1 <- `Unit.A`
y2 <- `Unit.B`
t.test(y1,y2,alternative = "two.sided",conf.level = 0.95,paired = TRUE)
attach(cutlet)
y1 <- `Unit.A`
y2 <- `Unit.B`
t.test(y1,y2,alternative = "two.sided",conf.level = 0.95,paired = TRUE)
var.test(cutlet$Unit.A,cutlet$Unit.B)#variance test
t.test(cutlet$Unit.A,cutlet$Unit.B,alternative = "two.sided",conf.level = 0.95,correct = TRUE)#two sample T.Test
t.test(cutlet$Unit.A,cutlet$Unit.B,alternative = "greater",var.equal = F)
plot(cutlet$Unit.A)
qqnorm(cutlet$Unit.A)
qqline(cutlet$Unit.A)
qqnorm(cutlet$Unit.B)
qqline(cutlet$Unit.B)
var(cutlet$Unit.A)
var(cutlet$Unit.B)
?var.test
var.test(cutlet$Unit.A,cutlet$Unit.B)#variance test 0.7053
0.08/0.11
0.08317945/0.117924
############2 sample T Test ##################
?t.test
############2 sample T Test ##################
t.test(cutlet$Unit.A,cutlet$Unit.B,alternative = "two.sided",conf.level = 0.95, var.equal = FALSE)#two sample T.Test
t.test(cutlet$Unit.A,cutlet$Unit.B,alternative = "greater",var.equal = FALSE)
t.test(cutlet$Unit.A,cutlet$Unit.B,alternative = "greater",conf.level = 0.95,var.equal = FALSE)
6.9642/7.01909
t.test(cutlet$Unit.A,cutlet$Unit.B,alternative = "two.sided",conf.level = 0.95, var.equal = FALSE)#two sample T.Test
Input <-read.csv(file.choose())    # cutlet.xlsx
View(Input)
colnames(Input)
View(Input)
attach(Input)
shapiro.test(Laboratory.1)
shapiro.test(Laboratory.2)
# p-value = 0.8637 >0.05 so p high null fly => It follows normal distribution
shapiro.test(Laboratory.3)
# p-value = 0.4205 >0.05 so p high null fly => It follows normal distribution
shapiro.test(Laboratory.4)
qqline(Laboratory.1)
qqline(Laboratory.2)
qqline(Laboratory.3)
qqline(Laboratory.4)
# p-value = 0.6619 >0.05 so p high null fly => It follows normal distribution
shapiro.test(Stacked_data)
# p-value = 0.6619 >0.05 so p high null fly => It follows normal distribution
shapiro.test(Input)
Stacklab=stack(Input)
# p-value = 0.6619 >0.05 so p high null fly => It follows normal distribution
shapiro.test(Stacklab)
attach(Stacklab)
shapiro.test(Laboratory.1)
# p-value = 0.5508 >0.05 so p high null fly => It follows normal distribution
shapiro.test(Laboratory.2)
# p-value = 0.8637 >0.05 so p high null fly => It follows normal distribution
shapiro.test(Laboratory.3)
# p-value = 0.4205 >0.05 so p high null fly => It follows normal distribution
shapiro.test(Laboratory.4)
# p-value = 0.6619 >0.05 so p high null fly => It follows normal distribution
shapiro.test(Stacklab)
View(Stacklab)
View(Stacklab)
attach(Stacklab)
# p-value = 0.6619 >0.05 so p high null fly => It follows normal distribution
shapiro.test(Stacklab)
attach(Input)
############################## problem  2
#Busines objective: Is there any difference in the average Turn Around Time (TAT)
# of reports of the laboratories on their preferred list
library(car)
stacklab=stack(Input)
shapiro.test(stacklab)
#Create Hypothesis for variances
#Ho= Variance of diameters of Unit A is equal to the variance of diameters of Unit B
#Ha= Variance of diameters of Unit A is not equal to the variance of diameters of Unit B
leveneTest(values~ ind, data = stacklab)
var(Laboratory.1)
var(Laboratory.2)
var(Laboratory.3)
var(Laboratory.4)
var.test(Laboratory.1,Laboratory.2,Laboratory.3,Laboratory.4)
var.test(Laboratory.1,Laboratory.2)
#Create Hypothesis for variances
#Ho= Variances are equal
#Ha= Variances are not equal
leveneTest(values~ ind, data = stacklab)
#Since p-value = 0.05161 > 0.05, we Accept null Hypothesis
# Variances are equal for all laboratory TAT data
# So we proceed with one way ANOVA test
############# ONE WAY ANOVA TEST ###############
Anova_results <- aov(values~ind,data = stacklab)
summary(Anova_results)
BuyersRatio <-read.csv(file.choose())    # BuyerRatio.csv
colnames(BuyersRatio)
View(BuyersRatio)
attach(BuyersRatio)
#############Normality test###############
Boxplot(East)
Boxplot(West)
Boxplot(North)
Boxplot(South)
DT=as.table(as.matrix(BuyersRatio))
DT
DT=as.table(as.matrix(BuyersRatio[,2:5]))
DT
chisq <- chisq.test(DT)
chisq
# p-value = 0.6603 > 0.05 so Accept null hypothesis
# So Buyers ratio across all region is same for male and Female
############################## problem  4
# Business objective : To determine whether the data is defective or not
# Here both variables are discrete and have more than 2 variables hence we choose chi square test.
# First free change the factor data into numeric data.
# here we have to test
# Ho : defective % are same at all the centers
# Ha: defective % varies  from center to center
BuyersRatio <-read.csv(file.choose())    # BuyerRatio.csv
# p-value = 0.6603 > 0.05 so Accept null hypothesis
# So Buyers ratio across all region is same for male and Female
############################## problem  4
# Business objective : To determine whether the data is defective or not
# Here both variables are discrete and have more than 2 variables hence we choose chi square test.
# First free change the factor data into numeric data.
# here we have to test
# Ho : defective % are same at all the centers
# Ha: defective % varies  from center to center
Customerorderform <-read.csv(file.choose())    # Customerorderform.csv
View(Customerorderform)
colnames(BuyersRatio)
colnames(Customerorderform)
table(Customerorderform)
COF=table(Customerorderform)
chisq.test(COF)
chisq.test(table(COF))
COF<-ifelse(Customerorderform=="Error Free",0,1)
View(COF)
COF1=table(COF)
chisq.test(COF1)
fantaloons<-read.csv(file.choose())
View(fantaloons)
attach(fantaloons)
tab<-table(fantaloons $Weekdays, fantaloons $Weekend)
tab
fantaloons<-read.csv(file.choose())
View(fantaloons)
tab<-table(Weekdays, Weekend)
tab
prop.test(x=c(66,167),n=c(233,167),conf.level = 0.95,correct = FALSE,alternative = "two.sided")
prop.test(tab,conf.level = 0.95,correct = FALSE,alternative = "two.sided")
prop.test(table(Weekdays, Weekend),conf.level = 0.95,correct = FALSE,alternative = "two.sided")
prop.test (table(Weekdays, Weekend), conf.level = 0.95,correct = FALSE,alternative = "less")
prop.test(table(Weekdays, Weekend),conf.level = 0.95,alternative = "two.sided")
prop.test(table(Weekdays, Weekend),conf.level = 0.95,correct = TRUE,alternative = "two.sided")
prop.test(table(Weekdays, Weekend),conf.level = 0.95,correct = FALSE,alternative = "two.sided")
library(readxl)
set1=read_xlsx(choose.files())
set1=read_xlsx(file.choose())
View(set1)
attach(set1)
mean(`Measure X`)
median(`Measure X`)
#mode
getmode <- function(x){
uniquv <- unique(x)
uniquv[which.max(tabulate(match(x,uniquv)))]
}
getmode(`Measure X`)
#Measures of skewness
install.packages("moments")
library(moments)
#Measures of Dispersion
var(`Measure X`)
sd(`Measure X`)
range(`Measure X`)
boxplot(`Measure X`)
bx = boxplot(Measure X,horizontal = TRUE)
bx = boxplot('Measure X',horizontal = TRUE)
boxplot(`Measure X`,horizontal = TRUE )
set1=read_xlsx(file.choose())
View(set1)
attach(set1)
mean(`Measure X`)
#Measures of Dispersion
var(`Measure X`)
sd(`Measure X`)
boxplot(`Measure X`,horizontal = TRUE )
bx = boxplot(`Measure X`,horizontal = TRUE)$out
bx$out
bx$out
library(moments)
bx = boxplot(`Measure X`,horizontal = TRUE)$out
bx$out
set1=as.data.frame(set1)
bx = boxplot(`Measure X`,horizontal = TRUE)$out
bx$out
class(set1)
library(shiny)
install.packages("shiny")
library(shiny)
bx = boxplot(`Measure X`,horizontal = TRUE)$out
bx
library(car)
bx = outlier.test(set1)
bx = outlierTest (set1)
bx = outlierTest (`Measure X`)
bx = outlierTest (`Measure X`)
set1=read_xlsx(file.choose())
set1=read_xlsx(file.choose())
set1=read_xlsx(file.choose())
View(set1)
bx = outlierTest (`Measure X`)
mod <- lm(`Measure X` ~ ., data=set1)
bx = outlierTest (mod)
bx
mod <- lm(`Measure X` ~ ., data=set1)
bx = outlierTest (mod)
bx
boxplot(`Measure X`,horizontal = TRUE )
bx$out
bx = boxplot(`Measure X`,horizontal = TRUE)$out
bx
1-(199^5/200^5)
set1=read_xlsx(file.choose())
View(set1)
set1=read_xlsx(file.choose())
View(set1)
set1=read_xlsx(file.choose())
View(set1)
attach(set1)
sd(x)
sd(X)
#Measures of Dispersion
var(X)
1-pnorm(50,45,8)
(30-38)/6
pnorm(30,38,6)
400*0.09121122
1.645*225/540
2.645*225/540
0.645*225/540
540-(1.645*225)
pnorm(0,5,3)
pnorm(0,7,4)
rocrpred<-prediction(prob,bank_data1$y)
# ROC Curve => used to evaluate the betterness of the logistic model
# more area under ROC curve better is the model
# We will use ROC curve for any classification technique not only for logistic
# install.packages("ROCR")
library(ROCR)
rocrpred<-prediction(prob,bank_data1$y)
library('AER')
library(plyr)
setwd("D:/Data_science/Assignments/Assignment_Logistic_Regression")
Bank_Data <- read.csv(file.choose()) # Choose the bank Data set
Bank_Data <- read.csv(file.choose()) # Choose the bank Data set
colnames(Bank_Data)
#bank_data1 <- as.data.frame(Bank_Data)
bank_data1=Bank_Data
summary(bank_data1)
table(bank_data1$y)
# Preparing a linear regression
mod_lm <- lm(y~.,data=bank_data1)
install.packages("splitstackshape")
View(bank_data1)
library(splitstackshape)
Bank_Data=cSplit(Bank_Data, 1:ncol(Bank_Data), sep=",", stripWhite=TRUE, type.convert=FALSE)
colnames(Bank_Data)
library(splitstackshape)
Bank_Data=cSplit(df, 1:ncol(Bank_Data), sep=",", stripWhite=TRUE, type.convert=FALSE)
Bank_Data=cSplit(Bank_Data, 1:ncol(Bank_Data), sep=",", stripWhite=TRUE, type.convert=FALSE)
colnames(Bank_Data)
colnames(Bank_Data)
Bank_Data=cSplit(Bank_Data, 1:ncol(Bank_Data), sep=",", stripWhite=TRUE, type.convert=FALSE)
colnames(Bank_Data)
Bank_Data=cSplit(Bank_Data, 1:ncol(Bank_Data), sep=";, stripWhite=TRUE, type.convert=FALSE)
colnames(Bank_Data)
#bank_data1 <- as.data.frame(Bank_Data)
bank_data1=Bank_Data
summary(bank_data1)
table(bank_data1$y)
# Preparing a linear regression
mod_lm <- lm(y~.,data=bank_data1)
pred1 <- predict(mod_lm,bank_data1)
# pred1
plot(age,pred1)
# We can also include NA values but where ever it finds NA value
# probability values obtained using the glm will also be NA
# So they can be either filled using imputation technique or
# exlclude those values
# GLM function use sigmoid curve to produce desirable results
# The output of sigmoid function lies in between 0-1
model <- glm(y~.,data=bank_data1,family = "binomial")
# To calculate the odds ratio manually we going r going to take exp of coef(model)
exp(coef(model))
# Confusion matrix table
prob <- predict(model,bank_data1,type="response")
summary(model)
# We are going to use NULL and Residual Deviance to compare the between different models
# Confusion matrix and considering the threshold value as 0.5
confusion<-table(prob>0.5,bank_data1$y)
confusion
# Model Accuracy
Accuracy<-sum(diag(confusion)/sum(confusion))
Accuracy # 90.18
# Creating empty vectors to store predicted classes based on threshold value
pred_values <- NULL
yes_no <- NULL
pred_values <- ifelse(prob>=0.5,1,0)
yes_no <- ifelse(prob>=0.5,"yes","no")
# Creating new column to store the above values
bank_data1[,"prob"] <- prob
bank_data1[,"pred_values"] <- pred_values
bank_data1[,"yes_no"] <- yes_no
# View(bank_data1[,c(1,31,36:38)])
table(bank_data1$y,bank_data1$pred_values)
# Calculate the below metrics
# precision | recall | True Positive Rate | False Positive Rate | Specificity | Sensitivity
# from the above table - 59
# ROC Curve => used to evaluate the betterness of the logistic model
# more area under ROC curve better is the model
# We will use ROC curve for any classification technique not only for logistic
# install.packages("ROCR")
library(ROCR)
rocrpred<-prediction(prob,bank_data1$y)
rocrperf<-performance(rocrpred,'tpr','fpr')
str(rocrperf)
plot(rocrperf,colorize=T,text.adj=c(-0.2,1.7))
# More area under the ROC Curve better is the logistic regression model obtained
## Getting cutt off or threshold value along with true positive and false positive rates in a data frame
str(rocrperf)
rocr_cutoff <- data.frame(cut_off = rocrperf@alpha.values[[1]],fpr=rocrperf@x.values,tpr=rocrperf@y.values)
colnames(rocr_cutoff) <- c("cut_off","FPR","TPR")
View(rocr_cutoff)
Bank_Data=cSplit(Bank_Data, 1:ncol(Bank_Data), sep=";", stripWhite=TRUE, type.convert=FALSE)
colnames(Bank_Data)
Bank_Data <- read.csv(file.choose()) # Choose the bank Data set
colnames(Bank_Data)
Bank_Data=cSplit(Bank_Data, 1:ncol(Bank_Data), sep=";", stripWhite=TRUE, type.convert=FALSE)
colnames(Bank_Data)
View(Bank_Data)
Bank_Data <- read.csv(file.choose()) # Choose the bank Data set
colnames(Bank_Data)
Bank_Data=cSplit(Bank_Data, 1:ncol, sep=";", stripWhite=TRUE, type.convert=FALSE)
Bank_Data=cSplit(df, 'Bank_Data', sep=";", type.convert=FALSE)
pnorm(0.046, 2000, )
1.96*1.96*(0.5*0.5*)/(0.0480.04)
1.96*1.96*(0.5*0.5)/(0.0480.04)
1.96*1.96*(0.5*0.5)/(0.04*0.04)
2.326*2.326*(0.5*0.5)/(0.04*0.04)
pnorm(55,50,40)-pnorm(45,50,40)
pnorm(55,50,4)-pnorm(45,50,4)
?pnorm
> pnorm(55,50,4)-pnorm(45,50,4)
[1] 0.7887005
> 1-0.7887005
[1] 0.2112995
qnorm(0.975)
0.71+(1.96*(0.71*(sqrt(1-0.71))))/100
0.71+(1.96*(0.71*(sqrt((1-0.71)/100)))
0.71+(1.96*(0.71*(sqrt((1-0.71)/100)))
0.71+(1.96*(0.71*(sqrt((1-0.71)/100))))
0.71+(1.96*(sqrt(0.71*(1-0.71)/100))))
0.71+(1.96*(sqrt(0.71*(1-0.71)/100)))
0.71-(1.96*(sqrt(0.71*(1-0.71)/100)))
0.8-(1.96*(sqrt(0.8*(1-0.8)/100)))
0.8+(1.96*(sqrt(0.8*(1-0.8)/100)))
0.55+(1.96*(sqrt(0.55*(1-0.55)/100)))
0.55-(1.96*(sqrt(0.55*(1-0.55)/100)))
0.55-(1.96*(sqrt(0.55*(1-0.55)/40)))
0.55+(1.96*(sqrt(0.55*(1-0.55)/40)))
0.91+(1.96*(sqrt(0.91*(1-0.91)/5250)))
0.91-(1.96*(sqrt(0.91*(1-0.91)/5250)))
0.6-(1.96*(sqrt(0.6*(1-0.6)/1000)))
0.6-(1.96*(sqrt(0.6*(1-0.6)/1000)))
0.6+(1.96*(sqrt(0.6*(1-0.6)/1000)))
