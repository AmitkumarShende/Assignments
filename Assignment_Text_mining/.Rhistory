table(salary_predictions, salary_test$Salary)
agreement <- salary_predictions == salary_test$Salary
table(agreement)
prop.table(table(agreement))
## Improving model performance ----rbfdot
salary_classifier_rbf <- ksvm(Salary ~ ., data = salary_train, kernel = "rbfdot")
salary_predictions_rbf <- predict(salary_classifier_rbf, salary_test)
agreement_rbf <- salary_predictions_rbf == salary_test$Salary
table(agreement_rbf)
prop.table(table(agreement_rbf))
## Improving model performance ----polydot
salary_classifier_polydot <- ksvm(Salary ~ ., data = salary_train, kernel = "polydot")
salary_predictions_polydot <- predict(salary_classifier_polydot, salary_test)
agreement_polydot <- salary_predictions_polydot == salary_test$Salary
table(agreement_polydot)
prop.table(table(agreement_polydot))
ForestFire= read.csv("D:/Data_science/Assignments/Assignment_SVM/forestfires.csv")
str(ForestFire)
class(ForestFire)
summary(ForestFire) # Confirms on the different scale and demands normalizing the data.
normalize<-function(x){
return ( (x-min(x))/(max(x)-min(x)))
}
ForestFire$temp = normalize(ForestFire$temp)
ForestFire$RH   = normalize(ForestFire$RH)
ForestFire$wind = normalize(ForestFire$wind)
ForestFire$rain = normalize(ForestFire$rain)
# Data Partition
set.seed(123)
ind <- sample(2, nrow(FF), replace = TRUE, prob = c(0.7,0.3))
ind <- sample(2, nrow(ForestFire), replace = TRUE, prob = c(0.7,0.3))
FF_train <- ForestFire[ind==1,]
FF_test  <- ForestFire[ind==2,]
library(e1071)
model1<-ksvm(size_category~temp+rain+wind+RH,
data= FF_train,kernel = "vanilladot")
model1
Area_pred <- predict(model1, FF_test)
table(Area_pred,FF_test$size_category)
agreement <- Area_pred == FF_test$size_category
table(agreement)
prop.table(table(agreement))
# kernel = rfdot
model_rfdot<-ksvm(size_category~temp+rain+wind+RH,
data= FF_train,kernel = "rbfdot")
pred_rfdot<-predict(model_rfdot,newdata=FF_test)
mean(pred_rfdot==FF_test$size_category) # 68.41
mean(Area_pred==FF_test$size_category)
model_poly<-ksvm(size_category~temp+rain+wind+RH,
data= FF_train,kernel = "polydot")
pred_poly<-predict(model_poly,newdata = FF_test)
mean(pred_poly==FF_test$size_category) # 67.80
install.packages("xlsx")
PMPG <- read.csv("D:/Data_science/Assignments/Cars.csv.csv")
PMPG <- read.csv("D:/Data_science/Assignments/Cars.csv")
View(PMPG)
P(MPG>38)
View(PMPG)
pnorm(MPG>38)
pnorm(PMPG$MPG>38)
library(dplyr)
library(psych)
mpg38=pnorm(PMPG$MPG > 38)
mpg38
mean(PMPG$MPG)
sd(PMPG$MPG)
median(PMPG$MPG)
P<-[PMPG$MPG > 38]
P<-[PMPG$MPG >= 38]
P1<- (PMPG$MPG >= 38)
P1
P1<- (PMPG$MPG > 38)
P1
mpg38=pnorm(P1,P1mean,P1sd)
P1mean<-mean(PMPG$MPG)
P1sd<-sd(PMPG$MPG)
median(PMPG$MPG)
P1<- (PMPG$MPG > 38)
P1
mpg38=pnorm(P1,P1mean,P1sd)
mpg38
P1median<-median(PMPG$MPG)
View(PMPG)
summary(PMPG)
library(ggplot2)
hist(PMPG$MPG,
main = "Histogram of MPG",
xlab = "MPG")
qqnorm(PMPG$MPG)
qqnorm(PMPG$MPG), qqline(PMPG$MPG)
qqnorm(PMPG$MPG); qqline(PMPG$MPG)
PMPG <- read.csv("D:/Data_science/Assignments/wc-at.csv")
View(PMPG)
qqnorm(PMPG$Waist); qqline(PMPG$Waist)
qqnorm(PMPG$AT); qqline(PMPG$AT)
p <- pt((260-270)/(90/sqrt(18)), 18)
pt(tscore,df)
?pt
mpg38=pnorm(P1,P1mean,P1sd)
P1<- PMPG$MPG > 38
P1
install.packages("Matrix")
library(recommenderlab)
library(dplyr)
library(caTools)
library(reshape2)
library(Matrix)
library(data.table)
library(ggplot2)
setwd("D:/Data_science/Assignments/Assignment_Recommandataion")
#book rating data
book_rate_data <- read.csv("D:/Data_science/Assignments/Assignment_Recommandataion/books.csv", header=TRUE)
book_rate_data <- book_rate_data[,-c(1)]   # removing column 1 as not required for analysis
names(book_rate_data)[1]<-"users" #.....[1]--column number, "users"----name to change for R
names(book_rate_data)[5]<-"ratings" #.....[5]--column number, "retings"----name to change for R
names(book_rate_data)[2]<-"bookname" #.....[2]--column number, "bookname"----name to change for R
names(book_rate_data)[3]<-"author" #.....[3]--column number, "author"----name to change for R
head(book_rate_data)
### Removing Duplicated Entries
book_rate_data<- book_rate_data[!duplicated(book_rate_data$bookname), ]
### Ordering tha data according to ratings
book_rate_data <- book_rate_data[order(book_rate_data[,5]), ]
### use only users with more than 0 ratings
book_rate_data <- filter(book_rate_data, book_rate_data$ratings >= 1)
#rating distribution
hist(book_rate_data$ratings)
## covert to matrix format
book_rate_data_g<-acast(book_rate_data, bookname ~ ratings)
### use only users with more than 0 ratings
book_rate_data <- filter(book_rate_data, book_rate_data$ratings >= 1)
### Removing Duplicated Entries
book_rate_data<- book_rate_data[!duplicated(book_rate_data$bookname), ]
#book rating data
book_rate_data <- read.csv("D:/Data_science/Assignments/Assignment_Recommandataion/books.csv", header=TRUE)
setwd("D:/Data_science/Assignments/Assignment_Recommandataion--------------------completed")
#book rating data
book_rate_data <- read.csv("D:/Data_science/Assignments/Assignment_Recommandataion/books.csv", header=TRUE)
#book rating data
book_rate_data <- read.csv("D:/Data_science/Assignments/Assignment_Recommandataion--------------------completed/books.csv", header=TRUE)
book_rate_data <- book_rate_data[,-c(1)]   # removing column 1 as not required for analysis
names(book_rate_data)[1]<-"users" #.....[1]--column number, "users"----name to change for R
names(book_rate_data)[5]<-"ratings" #.....[5]--column number, "retings"----name to change for R
names(book_rate_data)[2]<-"bookname" #.....[2]--column number, "bookname"----name to change for R
names(book_rate_data)[3]<-"author" #.....[3]--column number, "author"----name to change for R
head(book_rate_data)
### Removing Duplicated Entries
book_rate_data<- book_rate_data[!duplicated(book_rate_data$bookname), ]
### Ordering tha data according to ratings
book_rate_data <- book_rate_data[order(book_rate_data[,5]), ]
### use only users with more than 0 ratings
book_rate_data <- filter(book_rate_data, book_rate_data$ratings >= 1)
#rating distribution
hist(book_rate_data$ratings)
## covert to matrix format
book_rate_data_g<-acast(book_rate_data, bookname ~ ratings)
class(book_rate_data_g)
book_rate_dataM<-as.matrix(book_rate_data_g)
dim(book_rate_dataM)
book_rate_data_matrix <- as(book_rate_dataM, 'realRatingMatrix')
book_rate_data_matrix
as(book_rate_data_matrix, "list")
book_recomm_model1 <- Recommender(book_rate_data_matrix, method="POPULAR")
book_recomm_model2 = Recommender(book_rate_data_matrix, method="UBCF") ## User-based collaborative filtering
book_recomm_model3 = Recommender(book_rate_data_matrix, method="IBCF") ## Item-based collaborative filtering
#User Based Popular Filtering
#Predictions for two users
recommended_items1 <- predict(book_recomm_model1, book_rate_data_matrix, n=5)
R1<-as(recommended_items1, "list")
head(R1)
library(rvest)
library(XML)
library(magrittr)
library(tm)
library(wordcloud)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
library(devtools)
library(tm)
library(slam)
library(topicmodels)
install.packages("devtools")
library(devtools)
devtools::install_github("lchiffon/wordcloud2")
setwd("D:/Data_science/Assignments/Assignment_Text_mining")
# IMDBReviews #############################
aurl <- "https://www.imdb.com/title/tt1477834/reviews?ref_=tt_ov_rt"
IMDB_reviews <- NULL
for (i in 1:10){
murl <- read_html(as.character(paste(aurl,i,sep="=")))
rev <- murl %>%
html_nodes(".show-more__control") %>%
html_text()
IMDB_reviews <- c(IMDB_reviews,rev)
}
length(IMDB_reviews)
write.table(IMDB_reviews,"Aquaman.txt",row.names = F)
Aquaman <- read.delim('Aquaman.txt')
str(Aquaman)
View(Aquaman)
corpus <- iconv(Aquaman[-1,], to = "UTF-8")
head(corpus)
class(corpus)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
corpus <- tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus <- tm_map(corpus,removePunctuation)
inspect(corpus[1:5])
corpus <- tm_map(corpus,removeNumbers)
inspect(corpus[1:5])
cleanset<-tm_map(corpus,removeWords, stopwords('english'))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*','',x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
inspect(cleanset[1:5])
cleanset<-tm_map(cleanset,removeWords, c('can','film'))
inspect(cleanset[1:5])
cleanset<-tm_map(cleanset,removeWords, c('movie','movies'))
cleanset <- tm_map(cleanset, gsub,pattern = 'character', replacement = 'characters')
inspect(cleanset[1:5])
cleanset <- tm_map(cleanset,stripWhitespace)
inspect(cleanset[1:5])
#Term Document Matrix :
# Convert the unstructured data to structured data :
tdm <- TermDocumentMatrix(cleanset)
tdm
# the terms indicate that there are 1073 words and 479 documents(# of tweets) in this TDM
# Sparsity is 100% which indicates that there are lots of zero values.
tdm <- as.matrix(tdm)
tdm
tdm <- as.matrix(tdm)
tdm[1:10,1:20]
w <- rowSums(tdm)  # provides the no of times a particular word has been used.
w <- subset(w, w>= 25) # Pull words that were used more than 25 times.
barplot(w, las = 2, col = rainbow(50))
# Word Cloud :
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE) # Sort words in decreasing order.
set.seed(123)
# Word Cloud :
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE) # Sort words in decreasing order.
#set.seed(123)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
w <- rowSums(tdm)  # provides the no of times a particular word has been used.
w <- subset(w, w>= 25) # Pull words that were used more than 25 times.
barplot(w, las = 2, col = rainbow(50))
# Word Cloud :
library(wordcloud)
w <- sort(rowSums(tdm), decreasing = TRUE) # Sort words in decreasing order.
#set.seed(123)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
library(wordcloud2)
w <- data.frame(names(w),w)
windows()
colnames(w) <- c('word','freq')
wordcloud2(w,size = 0.5, shape = 'triangle', rotateRatio = 0.5,
minSize = 1)
letterCloud(w,word = 'A',frequency(5), size=1)
letterCloud(w,word = 'A',frequency(5), size=1)
# Read File
IMDB_reviews <- read.delim('Aquaman.TXT')
reviews <- as.character(IMDB_reviews[-1,])
class(reviews)
# Obtain Sentiment scores
s <- get_nrc_sentiment(reviews)
head(s)
reviews[4]
reviews[3]
reviews[3]
# Obtain Sentiment scores
s <- get_nrc_sentiment(reviews)
head(s)
reviews[3]
reviews[3]
reviews[4]
reviews[5]
reviews[6]
# Watched it!!! And have no words to describe the splendid performance of the cast and how beautifully wan visualised and presented this..This is the true jewel so far in DC universe<U+0001F44D><U+0001F3FB>\nWon't Add any Spoilers<U+0001F60B>..as this is something you should witness"
# on tweet 4, you have 4 for anger, each one work for disgust, fear
# and sadness, 3 for trust , 4 words for negative and 2 positive.
get_nrc_sentiment('stunning')
# Watched it!!! And have no words to describe the splendid performance of the cast and how beautifully wan visualised and presented this..This is the true jewel so far in DC universe<U+0001F44D><U+0001F3FB>\nWon't Add any Spoilers<U+0001F60B>..as this is something you should witness"
# on tweet 4, you have 4 for anger, each one work for disgust, fear
# and sadness, 3 for trust , 4 words for negative and 2 positive.
get_nrc_sentiment('twists')
head(s)
# I have no idea why som6ebody would give this movie a good score, let alone 10. After 20 minutes I have decided that its for the best to switch my brain off and enjoy the show
# on review 6, you have 4 for anger,4 each for anti, fear, surprise and 5 for sadness,7 for trust , 11 words for negative and 9 positive.
get_nrc_sentiment('twists')
barplot(colSums(s), las = 2.5, col = rainbow(10),
ylab = 'Count',main= 'Sentiment scores for amazon Reviews
for Apple Macbook')
barplot(colSums(s), las = 2.5, col = rainbow(10),
ylab = 'Count',main= 'Sentiment scores for IMDB Reviews
for Aquaman')
install.packages("rclinicaltrials")
library(rvest)
library(XML)
library(magrittr)
library(tm)
library(wordcloud)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
library(devtools)
library(tm)
library(slam)
library(topicmodels)
library(rclinicaltrials)
library(devtools
install.packages("rclinicaltrials")
install.packages("devtools")
install.packages("devtools")
install.packages("rclinicaltrials")
install.packages("devtools")
install.packages("devtools")
library(devtools)
devtools::install_github("lchiffon/wordcloud2")
library(rclinicaltrials)
install_github("sachsmc/rclinicaltrials")
library(rclinicaltrials)
devtools::install_github("lchiffon/wordcloud2")
install.packages("devtools")
install.packages("devtools")
library(devtools)
devtools::install_github("lchiffon/wordcloud2")
1library(wordcloud2)
install.packages("wordcloud2")
install.packages("wordcloud2")
1library(wordcloud2)
library(wordcloud2)
z <- clinicaltrials_download(query = 'Hepatocellular Carcinoma', count = 10, include_results = TRUE)
str(z)
zstudyinfo <- z$study_information$study_info
zoutcomes <- z$study_information$outcomes
zinterventions <- z$study_information$interventions
zoutcomes <- as.character(zoutcomes)
zinterventions <- as.character(zinterventions)
zhcc <- rbind(zoutcomes,zinterventions)
write.table(zhcc,"zhcc.txt",row.names = F)
write.table(zinterventions,"inv.txt",row.names = F)
write.table(zoutcomes,"out.txt",row.names = F)
hcc <- read.delim('out.txt')
str(hcc)
# Build Corpus and DTM/TDM
library(tm)
corpus <- hcc[-1,]
head(corpus)
class(corpus)
rpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
inspect(corpus[1:5])
corpus <- tm_map(corpus,removePunctuation)
inspect(corpus[1:5])
corpus <- tm_map(corpus,removeNumbers)
inspect(corpus[1:5])
cleanset<-tm_map(corpus,removeWords, stopwords('english'))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*','',x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
inspect(cleanset[1:5])
library(rvest)
library(XML)
library(magrittr)
library(tm)
library(wordcloud)
library(syuzhet)
library(lubridate)
library(ggplot2)
library(scales)
library(reshape2)
library(dplyr)
library(devtools)
library(tm)
library(slam)
library(topicmodels)
install.packages("devtools")
install.packages("devtools")
install.packages("wordcloud2")
install.packages("wordcloud2")
library(devtools)
devtools::install_github("lchiffon/wordcloud2")
library(wordcloud2)
install_github("sachsmc/rclinicaltrials")
library(rclinicaltrials)
setwd("D:/Data_science/Assignments/Assignment_Text_mining")
z <- clinicaltrials_download(query = 'Hepatocellular Carcinoma', count = 10, include_results = TRUE)
View(z)
str(z)
zstudyinfo <- z$study_information$study_info
zoutcomes <- z$study_information$outcomes
zinterventions <- z$study_information$interventions
zoutcomes <- as.character(zoutcomes)
zinterventions <- as.character(zinterventions)
zhcc <- rbind(zoutcomes,zinterventions)
write.table(zhcc,"zhcc.txt",row.names = F)
write.table(zinterventions,"inv.txt",row.names = F)
write.table(zoutcomes,"out.txt",row.names = F)
hcc <- read.delim('out.txt')
str(hcc)
# Build Corpus and DTM/TDM
library(tm)
corpus <- hcc[-1,]
head(corpus)
class(corpus)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
# Clean the text
corpus <- tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus <- tm_map(corpus,removePunctuation)
inspect(corpus[1:5])
corpus <- tm_map(corpus,removeNumbers)
inspect(corpus[1:5])
cleanset<-tm_map(corpus,removeWords, stopwords('english'))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*','',x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
inspect(cleanset[1:5])
cleanset<-tm_map(cleanset,removeWords, c('nct','cnct'))
inspect(cleanset[1:5])
cleanset <- tm_map(cleanset,stripWhitespace)
inspect(cleanset[1:5])
#Term Document Matrix :
# Convert the unstructured data to structured data :
tdm <- TermDocumentMatrix(cleanset)
tdm
corpus <- iconv(hcc[-1,], to = "UTF-8")
head(corpus)
class(corpus)
corpus <- Corpus(VectorSource(corpus))
inspect(corpus[1:5])
# Clean the text
corpus <- tm_map(corpus,tolower)
inspect(corpus[1:5])
corpus <- tm_map(corpus,removePunctuation)
inspect(corpus[1:5])
corpus <- tm_map(corpus,removeNumbers)
inspect(corpus[1:5])
cleanset<-tm_map(corpus,removeWords, stopwords('english'))
inspect(cleanset[1:5])
removeURL <- function(x) gsub('http[[:alnum:]]*','',x)
cleanset <- tm_map(cleanset, content_transformer(removeURL))
inspect(cleanset[1:5])
cleanset<-tm_map(cleanset,removeWords, c('nct','cnct'))
inspect(cleanset[1:5])
cleanset <- tm_map(cleanset,stripWhitespace)
inspect(cleanset[1:5])
#Term Document Matrix :
# Convert the unstructured data to structured data :
tdm <- TermDocumentMatrix(cleanset)
tdm
# the terms indicate that there are 276 words and 4 documents in this TDM
# Sparsity is 74% which indicates that there are zero values
tdm <- as.matrix(tdm)
tdm[1:10,1:4]
w <- rowSums(tdm)  # provides the no of times a particular word has been used.
w <- subset(w, w>= 3) # Pull words that were used more than 2 times.
barplot(w, las = 2, col = rainbow(50))
w <- subset(w, w>= 5) # Pull words that were used more than 2 times.
barplot(w, las = 2, col = rainbow(50))
# Word Cloud :
w <- sort(rowSums(tdm), decreasing = TRUE) # Sort words in decreasing order.
#set.seed(123)
wordcloud(words = names(w), freq = w,
max.words = 250,random.order = F,
min.freq =  3,
colors = brewer.pal(8, 'Dark2'),
scale = c(5,0.3),
rot.per = 0.6)
w <- data.frame(names(w),w)
colnames(w) <- c('word','freq')
wordcloud2(w,size = 0.5, shape = 'triangle', rotateRatio = 0.5,
minSize = 1)
# Read File
HCC_Outcome <- read.delim('cleanset.TXT')
# Read File
HCC_Outcome <- read.delim('cleanset.TXT')
# Read File
HCC_Outcome <- read.delim('out.TXT')
trail_out <- as.character(HCC_Outcome[-1,])
View(HCC_Outcome)
class(trail_out)
# Obtain Sentiment scores
s <- get_nrc_sentiment(trail_out)
head(s)
get_nrc_sentiment('diagnosis')
get_nrc_sentiment('diagnosis')
# diagnosis has 1 anticipation, 1 fear, 1 trust and 1 negative value
get_nrc_sentiment('death')
barplot(colSums(s), las = 2.5, col = rainbow(10),
ylab = 'Count',main= 'Sentiment scores for HCC Clinical Trail')
